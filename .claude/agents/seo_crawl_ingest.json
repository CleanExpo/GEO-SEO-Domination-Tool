{
  "version": "1.0",
  "created": "2025-10-02",
  "compat": {
    "shell": "powershell",
    "os": "windows",
    "editors": ["vscode"],
    "cli": ["claude-code-cli", "cline"]
  },
  "conventions": {
    "language": "TypeScript",
    "frameworks": ["Next.js 15"],
    "repo_layout": "src/**, app router"
  },
  "name": "SEO Crawl & Ingest",
  "id": "seo_crawl_ingest",
  "category": "data-ingest",
  "summary": "Crawls allowed pages, extracts title/H1/meta description, and writes a local content index for SEMRUSH comparisons.",
  "system_prompt": "üï∑Ô∏è I crawl start URLs (respecting allowed host filters), fetch HTML, extract <title>, first <h1>, and <meta name='description'>, then write /data/content/index.json. I'm conservative, stay within host, and skip assets. Task ends when index.json exists and parses.",
  "goals": [
    "Fetch a limited set of pages within the allowed host",
    "Extract title, h1, description using simple HTML regex",
    "Write a normalized index to /data/content/index.json"
  ],
  "stop_conditions": {
    "max_iterations": 1,
    "success_checks": [
      "data/content/index.json exists",
      "JSON parses and includes at least one entry"
    ]
  },
  "inputs_schema": {
    "type": "object",
    "properties": {
      "projectPath": { "type": "string" },
      "startUrls": { "type": "array", "items": { "type": "string" } },
      "allowedHost": { "type": "string", "description": "Only crawl URLs whose host ends with this value" },
      "maxPages": { "type": "number", "default": 10 }
    },
    "required": ["projectPath", "startUrls", "allowedHost"]
  },
  "outputs_schema": {
    "type": "object",
    "properties": {
      "siteMapPath": { "type": "string" },
      "contentIndexPath": { "type": "string" },
      "pagesCrawled": { "type": "number" },
      "metadataExtracted": {
        "type": "object",
        "properties": {
          "titlesFound": { "type": "number" },
          "h1sFound": { "type": "number" },
          "descriptionsFound": { "type": "number" }
        }
      }
    },
    "required": ["contentIndexPath"]
  },
  "tools": {
    "commands": [
      {
        "name": "crawl_and_extract",
        "description": "PowerShell mini-crawler with basic extraction",
        "cmd": "pwsh -NoProfile -Command \"$outDir='${projectPath}\\data\\content'; New-Item -Force -ItemType Directory $outDir | Out-Null; $visited=@{}; $queue = New-Object System.Collections.Queue; ${startUrls} | ForEach-Object { $queue.Enqueue($_) }; $results=@(); $limit=[int]${maxPages}; $i=0; while($queue.Count -gt 0 -and $i -lt $limit){ $url=$queue.Dequeue(); try { $uri=[Uri]$url } catch { continue }; if (-not ($uri.Host -like '*${allowedHost}')) { continue }; if ($visited.ContainsKey($url)) { continue } $visited[$url]=$true; try { $r = Invoke-WebRequest -Uri $url -UseBasicParsing -TimeoutSec 20 } catch { continue }; $html = $r.Content; $title = [regex]::Match($html, '<title>(.*?)</title>', 'IgnoreCase').Groups[1].Value; $h1 = [regex]::Match($html, '<h1[^>]*>(.*?)</h1>', 'IgnoreCase').Groups[1].Value; $desc = [regex]::Match($html, '<meta[^>]*name=[\"'']description[\"''][^>]*content=[\"''](.*?)[\"'']', 'IgnoreCase').Groups[1].Value; $results += [pscustomobject]@{ url=$url; title=$title; h1=$h1; description=$desc }; $i++; $links = ([regex]::Matches($html, 'href=[\"''](.*?)[\"'']')).Groups[1] | ForEach-Object { $_.Value } | Where-Object { $_ -like 'http*' }; foreach($lnk in $links){ try { $lnkUri=[Uri]$lnk } catch { continue }; if ($lnkUri.Host -like '*${allowedHost}' -and -not $visited.ContainsKey($lnk)) { $queue.Enqueue($lnk) } } }; $json = $results | ConvertTo-Json -Depth 6; Set-Content -Encoding UTF8 -Path (Join-Path $outDir 'index.json') -Value $json; Set-Content -Encoding UTF8 -Path (Join-Path $outDir 'sitemap.txt') -Value ($results | ForEach-Object { $_.url } | Sort-Object -Unique)\""
      },
      {
        "name": "validate_index",
        "description": "Ensure index.json parses and has entries",
        "cmd": "pwsh -NoProfile -Command \"$p='${projectPath}\\data\\content\\index.json'; $d = Get-Content $p -Raw | ConvertFrom-Json; if (-not $d -or $d.Count -lt 1) { throw 'No entries in content index' }\""
      }
    ]
  },
  "workflow": [
    {
      "step": 1,
      "name": "Initialize Crawler",
      "tasks": [
        "Create data/content directory",
        "Initialize visited URLs tracking",
        "Initialize queue with start URLs",
        "Set crawl limits (maxPages)"
      ]
    },
    {
      "step": 2,
      "name": "Crawl Pages",
      "tasks": [
        "Dequeue URL from queue",
        "Validate URL matches allowed host",
        "Fetch HTML with timeout protection",
        "Track visited URLs to avoid duplicates"
      ]
    },
    {
      "step": 3,
      "name": "Extract Metadata",
      "tasks": [
        "Extract <title> content with regex",
        "Extract first <h1> content",
        "Extract <meta name='description'> content",
        "Store extracted data in results array"
      ]
    },
    {
      "step": 4,
      "name": "Discover Links",
      "tasks": [
        "Find all href attributes in HTML",
        "Filter for HTTP/HTTPS links only",
        "Validate links match allowed host",
        "Add new links to crawl queue"
      ]
    },
    {
      "step": 5,
      "name": "Save Results",
      "tasks": [
        "Convert results to JSON",
        "Save to data/content/index.json",
        "Generate sitemap.txt with all URLs",
        "Validate JSON parses correctly"
      ]
    }
  ],
  "plan": [
    "Run crawl_and_extract (bounded by maxPages and allowedHost)",
    "Run validate_index",
    "Return paths to sitemap.txt and index.json"
  ],
  "crawl_strategy": {
    "method": "breadth_first",
    "description": "Uses queue-based BFS to crawl pages level by level",
    "constraints": {
      "maxPages": "Hard limit on total pages crawled",
      "allowedHost": "Only crawl URLs within this host/domain",
      "timeout": "20 seconds per page fetch",
      "deduplication": "Track visited URLs to avoid loops"
    }
  },
  "extraction_patterns": {
    "title": {
      "regex": "<title>(.*?)</title>",
      "flags": "IgnoreCase",
      "description": "Extracts content between <title> tags"
    },
    "h1": {
      "regex": "<h1[^>]*>(.*?)</h1>",
      "flags": "IgnoreCase",
      "description": "Extracts first H1 heading content"
    },
    "meta_description": {
      "regex": "<meta[^>]*name=[\"']description[\"'][^>]*content=[\"'](.*?)[\"']",
      "flags": "IgnoreCase",
      "description": "Extracts meta description content attribute"
    },
    "links": {
      "regex": "href=[\"'](.*?)[\"']",
      "filter": "http* URLs only",
      "description": "Extracts all href links for crawling"
    }
  },
  "output_format": {
    "index.json": {
      "structure": "Array of page objects",
      "fields": {
        "url": "Full URL of the page",
        "title": "Page title from <title> tag",
        "h1": "First H1 heading content",
        "description": "Meta description content"
      },
      "example": [
        {
          "url": "https://example.com/page1",
          "title": "Example Page 1",
          "h1": "Welcome to Page 1",
          "description": "This is an example page description"
        }
      ]
    },
    "sitemap.txt": {
      "structure": "Newline-separated URLs",
      "sorted": true,
      "deduplicated": true
    }
  },
  "seo_use_cases": [
    "Content audit - analyze all page titles and H1s",
    "Meta description coverage - find missing descriptions",
    "Title tag optimization - compare with SEMrush keywords",
    "Site structure analysis - understand internal linking",
    "Duplicate content detection - find similar titles/H1s",
    "Sitemap generation - create URL list for indexing"
  ],
  "best_practices": [
    "Always set maxPages to avoid infinite crawls",
    "Use allowedHost to stay within target domain",
    "Respect robots.txt (not implemented in this basic version)",
    "Add delays between requests for production use",
    "Handle redirects and canonical URLs",
    "Clean HTML entities from extracted text",
    "Log failed fetches for debugging",
    "Validate extracted data isn't empty"
  ],
  "limitations": {
    "no_javascript": "Only fetches static HTML, no JS rendering",
    "basic_regex": "Simple regex extraction, not full HTML parsing",
    "no_robots_txt": "Doesn't check robots.txt restrictions",
    "no_rate_limiting": "No built-in delays between requests",
    "no_authentication": "Can't crawl password-protected pages",
    "no_retry_logic": "Failed fetches are skipped"
  },
  "enhancement_suggestions": [
    "Add cheerio/jsdom for proper HTML parsing",
    "Implement robots.txt checking",
    "Add configurable rate limiting",
    "Support authentication headers",
    "Extract additional SEO elements (canonical, og tags)",
    "Calculate content quality metrics",
    "Detect broken internal links",
    "Export to CSV or Excel format"
  ],
  "integration_examples": {
    "semrush_comparison": {
      "description": "Compare crawled titles with SEMrush keyword data",
      "workflow": [
        "Run seo_crawl_ingest to get page titles",
        "Run semrush_analytical_finder to get keyword data",
        "Compare title keywords with ranking keywords",
        "Identify optimization opportunities"
      ]
    },
    "content_gap_analysis": {
      "description": "Find missing pages or topics",
      "workflow": [
        "Crawl competitor site with seo_crawl_ingest",
        "Crawl your site with same agent",
        "Compare sitemaps to find gaps",
        "Create new content based on gaps"
      ]
    }
  }
}
